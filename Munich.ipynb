{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%install_ext https://raw.github.com/cpcloud/ipython-autotime/master/autotime.py\n",
    "%load_ext autotime\n",
    "import osmnx as ox, geopandas as gpd, pandas as pd, networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPolygon\n",
    "from IPython.display import Image\n",
    "from descartes import PolygonPatch\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "%matplotlib inline\n",
    "ox.config(log_console=True, use_cache=True)\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import libpysal as lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = ox.gdf_from_place('Munich, Germany')\n",
    "city = ox.project_gdf(city)\n",
    "fig, ax = ox.plot_shape(city, figsize=(4,4))\n",
    "ox.save_gdf_shapefile(city)\n",
    "#crs = {'init': 'epsg:4326'}\n",
    "#city = city.to_crs(crs)\n",
    "print(city.crs)\n",
    "city.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Airbnb_Munich.csv', header=0)\n",
    "data = data[data.room_type=='Entire home/apt']\n",
    "data.dropna()\n",
    "data = data[['id','latitude', 'longitude', 'price']]\n",
    "print(len(data))\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(\"data max index:\", data.index.max())\n",
    "print(\"number of NA prices:\", data.price.isna().sum())\n",
    "\n",
    "#data.loc[data.bedrooms == 0, 'bedrooms'] = 1\n",
    "\n",
    "# price_proom = np.divide(np.array(data[pd.notnull(data['bedrooms'])].price),np.array(data[pd.notnull(data['bedrooms'])].bedrooms))\n",
    "# mean_bed = np.mean(price_proom)\n",
    "# print(\"average bedroom price\",mean_bed)\n",
    "\n",
    "# nan_list = data[data.price.isna()].index.tolist()\n",
    "# data.loc[data.bedrooms.isna(), 'bedrooms'] = np.ceil(data.loc[data.bedrooms.isna(), 'price']/mean_bed)\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(data.longitude, data.latitude)]\n",
    "#data = data.drop(['longitude', 'latitude'], axis=1)\n",
    "crs = {'init': 'epsg:4326'}\n",
    "gdf = gpd.GeoDataFrame(data, crs=crs, geometry=geometry)\n",
    "\n",
    "meter_500 = 0.00899928/2\n",
    "\n",
    "\n",
    "# make the geometry a multipolygon if it's not already\n",
    "geometry = city['geometry'].iloc[0]\n",
    "if isinstance(geometry, Polygon):\n",
    "    geometry = MultiPolygon([geometry])\n",
    "    \n",
    "# quadrat_width is in the units the geometry is in, so we'll do a tenth of a degree\n",
    "geometry_cut = ox.quadrat_cut_geometry(geometry, quadrat_width=meter_500)\n",
    "\n",
    "city['coords'] = city['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "city['coords'] = [coords[0] for coords in city['coords']]\n",
    "\n",
    "polylist = [p for p in geometry_cut]\n",
    "\n",
    "polyframe = gpd.GeoDataFrame(geometry=polylist)\n",
    "polyframe.crs = city.geometry.crs\n",
    "polyframe['center_lon'] = polyframe['geometry'].apply(lambda x: x.centroid.coords[0][0])\n",
    "polyframe['center_lat'] = polyframe['geometry'].apply(lambda x: x.centroid.coords[0][1])\n",
    "\n",
    "\n",
    "pointInPoly = gpd.sjoin(polyframe, gdf, op='contains')\n",
    "print(\"pointinpoly length:\",len(pointInPoly))\n",
    "\n",
    "#pointInPoly.sort_values(['PlateID', 'Time'], inplace=True)\n",
    "pointInPoly['index'] = pointInPoly.index\n",
    "pointInPoly.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "present_ind = list(pointInPoly['index_right'])\n",
    "gdf_test_dropped = gdf.iloc[present_ind,:]\n",
    "gdf_dropped = gdf_test_dropped.copy()\n",
    "gdf_dropped.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"check contains:\",polyframe.iloc[pointInPoly.loc[len(pointInPoly)-1, 'index']].geometry.contains(gdf_dropped.loc[len(pointInPoly)-1,'geometry']))\n",
    "gdf_dropped['pindex'] = pointInPoly['index']\n",
    "\n",
    "print(\"check if there are NAs:\", gdf_dropped.isna().values.any())\n",
    "\n",
    "pindex = gdf_dropped.pindex.unique()\n",
    "airbnb_dict = dict(gdf_dropped.pindex.value_counts())\n",
    "counts = pd.DataFrame(list(airbnb_dict.items()), columns=['key', 'count'])\n",
    "counts = counts[counts['count']>7]\n",
    "counts = counts.copy()\n",
    "airbnb_dict = dict(zip(list(counts['key']), list(counts['count'])))\n",
    "polyair = polyframe.copy()\n",
    "polyair['count'] = 0\n",
    "polyair['count'].update(pd.Series(airbnb_dict))\n",
    "\n",
    "# plot the city\n",
    "west, south, east, north = city.unary_union.bounds\n",
    "fig, ax = plt.subplots(figsize=(40,26))\n",
    "\n",
    "polyframe.plot(ax=ax, color='#000004')\n",
    "polyair.plot(column='count',  legend=True, cmap='magma', ax=ax)    \n",
    "ax.set_xlim(west, east)\n",
    "ax.set_ylim(south, north)\n",
    "#ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "gdf_dropped = gdf_dropped[gdf_dropped.pindex.isin(airbnb_dict.keys())]\n",
    "gdf_dropped = gdf_dropped.copy()\n",
    "gdf_dropped = gdf_dropped[gdf_dropped['price'] > 0]\n",
    "gdf_dropped = gdf_dropped.copy()\n",
    "#gdf_dropped['price'] = gdf_dropped.price.divide(gdf_dropped.bedrooms)\n",
    "gdf_dropped = gdf_dropped[gdf_dropped.price<400]\n",
    "gdf_dropped = gdf_dropped.copy()\n",
    "gdf_dropped.reset_index(drop=True, inplace=True)\n",
    "gdf_dropped.price.hist(bins=100)\n",
    "\n",
    "prices_dict = dict(gdf_dropped.groupby('pindex')['price'].mean())\n",
    "from scipy import stats\n",
    "hmean_prices_dict = dict(gdf_dropped.groupby('pindex')['price'].apply(lambda x: stats.hmean(x)))\n",
    "median_prices_dict = dict(gdf_dropped.groupby('pindex')['price'].median())\n",
    "\n",
    "polyair['prices'] = 0\n",
    "polyair['hmean_prices'] = 0\n",
    "polyair['median_prices'] = 0\n",
    "polyair['prices'].update(pd.Series(prices_dict))\n",
    "polyair['hmean_prices'].update(pd.Series(hmean_prices_dict))\n",
    "polyair['median_prices'].update(pd.Series(median_prices_dict))\n",
    "\n",
    "\n",
    "# plot the city\n",
    "west, south, east, north = city.unary_union.bounds\n",
    "fig, ax = plt.subplots(figsize=(40,26))\n",
    "\n",
    "polyframe.plot(ax=ax, color='#000004')\n",
    "polyair.plot(column='prices',  legend=True, cmap='magma', ax=ax)    \n",
    "ax.set_xlim(west, east)\n",
    "ax.set_ylim(south, north)\n",
    "#ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# plot the city\n",
    "west, south, east, north = city.unary_union.bounds\n",
    "fig, ax = plt.subplots(figsize=(40,30))\n",
    "\n",
    "\n",
    "polyframe.plot(ax=ax, color='#000004')\n",
    "polyair.plot(column='hmean_prices',  legend=True, cmap='magma', ax=ax)    \n",
    "ax.set_xlim(west, east)\n",
    "ax.set_ylim(south, north)\n",
    "#ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the city\n",
    "west, south, east, north = city.unary_union.bounds\n",
    "fig, ax = plt.subplots(figsize=(40,20))\n",
    "\n",
    "\n",
    "polyframe.plot(ax=ax, color='#000004')\n",
    "polyair.plot(column='median_prices',  legend=True, cmap='magma', ax=ax)    \n",
    "ax.set_xlim(west, east)\n",
    "ax.set_ylim(south, north)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# MatPlotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#x, y = np.array(polyair.center_lon), np.array(polyair.center_lat)\n",
    "\n",
    "x = np.array(polyair.center_lon)*500/meter_500\n",
    "x = x-x.min()\n",
    "x = x/1000\n",
    "\n",
    "y = np.array(polyair.center_lat)*500/meter_500 \n",
    "y = y-y.min()\n",
    "y = y/1000\n",
    "\n",
    "\n",
    "# Plot the 3D figure of the fitted function and the residuals.\n",
    "fig = plt.figure(figsize=(14,12))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(x, y, np.array(polyair.hmean_prices), cmap='plasma', s=3)\n",
    "ax.set_zlim(0,np.max(np.array(polyair.hmean_prices))+2)\n",
    "ax.view_init(30, 300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "scat = ax.scatter(x, y, c=np.array(polyair.hmean_prices), s=10, cmap='hot')\n",
    "cbar = fig.colorbar(scat)\n",
    "#cbar.set_clim(0, 250)\n",
    "plt.savefig('images/Munich_observed.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the city\n",
    "west, south, east, north = city.unary_union.bounds\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(40,30))\n",
    "for polygon, n in zip(geometry_cut, np.arange(len(polylist))):\n",
    "    p = polygon.representative_point().coords[:][0]\n",
    "    patch = PolygonPatch(polygon, fc='#ffffff', ec='#000000', alpha=0.5)\n",
    "    ax.add_patch(patch)\n",
    "    plt.annotate(s=n, xy=p,\n",
    "                 horizontalalignment='center', size=9)\n",
    "polyframe.plot(ax=ax, color='#000004', alpha=0.5)\n",
    "polyair.plot(column='hmean_prices',  legend=True, cmap='magma', ax=ax, alpha=0.7, zorder=2)    \n",
    "ax.set_xlim(west, east)\n",
    "ax.set_ylim(south, north)\n",
    "#ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyair.loc[polyair.index==1172, 'median_prices'] = 42\n",
    "polyair.loc[polyair.index==1172, 'hmean_prices'] = 42\n",
    "polyair.loc[polyair.index==1172, 'prices'] = 42\n",
    "\n",
    "polyair.loc[polyair.index==1055, 'median_prices'] = 41\n",
    "polyair.loc[polyair.index==1055, 'hmean_prices'] = 41\n",
    "polyair.loc[polyair.index==1055, 'prices'] = 41\n",
    "\n",
    "polyair.loc[polyair.index==1624, 'median_prices'] = 88\n",
    "polyair.loc[polyair.index==1624, 'hmean_prices'] = 88\n",
    "polyair.loc[polyair.index==1624, 'prices'] = 88\n",
    "\n",
    "polyair.loc[polyair.index==1897, 'median_prices'] = 31\n",
    "polyair.loc[polyair.index==1897, 'hmean_prices'] = 31\n",
    "polyair.loc[polyair.index==1897, 'prices'] = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the city\n",
    "west, south, east, north = city.unary_union.bounds\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(45,32))\n",
    "for polygon, n in zip(geometry_cut, np.arange(len(polylist))):\n",
    "    p = polygon.representative_point().coords[:][0]\n",
    "    patch = PolygonPatch(polygon, fc='#ffffff', ec='#000000', alpha=0.5)\n",
    "    ax.add_patch(patch)\n",
    "    plt.annotate(s=n, xy=p,\n",
    "                 horizontalalignment='center', size=9)\n",
    "polyframe.plot(ax=ax, color='#000004', alpha=0.5)\n",
    "polyair.plot(column='hmean_prices',  legend=True, cmap='magma', ax=ax, alpha=0.7, zorder=2)    \n",
    "ax.set_xlim(west, east)\n",
    "ax.set_ylim(south, north)\n",
    "#ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = polyair[polyair.index.isin([1399, 1608, 1595, 1422, 767, 934])]\n",
    "cdf[\"cen_lon_km\"] = cdf['center_lon'].apply(lambda x: (x*500/meter_500 - (polyair.center_lon*500/meter_500).min())/1000)\n",
    "cdf[\"cen_lat_km\"] = cdf['center_lat'].apply(lambda x: (x*500/meter_500 - (polyair.center_lat*500/meter_500).min())/1000)\n",
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {}\n",
    "xy = np.vstack([x, y])\n",
    "zobs = np.array(polyair.hmean_prices)\n",
    "#zobs = np.where(zobs>0, zobs, 0.1*zobs.max())\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _gaussian(M, *args):\n",
    "    xy = M\n",
    "    arr = np.zeros(len(zobs))\n",
    "    for i in range(len(args)//7):\n",
    "        arr += twoD_Gaussian_alpha(xy, *args[i*7:i*7+7])\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "guess_prms = [#(105.42, 19.375937,9.841730, 3, 2.5,0.1, 1.5),\n",
    "              #(105.15, 24.343842,11.321942,3, 3,1.3, 1.2),\n",
    "              (165.49,22.356680, 8.361518, 2.5, 2,1.42, 1.1),\n",
    "#               (138.68,25.337423,7.868114, 3, 3,2.19, 1.5),\n",
    "#               (107.42,25.834213,4.907690, 1.48,0.72,7.48, 1.3),\n",
    "#                 (119.15,19.375937,4.907690, 1.48,0.72,7.48, 1.3)\n",
    "                #(170,24.49698,16.784035, 1.48,0.72,7.48, 1.3)\n",
    "             ]\n",
    "\n",
    "# Flatten the initial guess parameter list.\n",
    "p0 = [p for prms in guess_prms for p in prms]\n",
    "\n",
    "def twoD_Gaussian_alpha(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, alpha):\n",
    "    x, y = xy\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)    \n",
    "    a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)\n",
    "    b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)\n",
    "    c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)\n",
    "    g = amplitude*np.exp( - (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo) \n",
    "                            + c*((y-yo)**2))**alpha)\n",
    "    return g.ravel()\n",
    "popt, pcov = opt.curve_fit(_gaussian, xy, zobs, p0)\n",
    "#pred_params, uncert_cov = opt.curve_fit(gauss2d, xy, zobs, p0=guess)\n",
    "\n",
    "zpred = np.zeros(len(zobs))\n",
    "for i in range(len(popt)//7):\n",
    "    zpred += twoD_Gaussian_alpha(xy, *popt[i*7:i*7+7])\n",
    "\n",
    "# for i in range(len(popt)//5):\n",
    "#     fit += gaussian(X, Y, *popt[i*5:i*5+5])\n",
    "print('Fitted parameters:')\n",
    "print(popt)\n",
    "\n",
    "rms = np.sqrt(np.mean((zobs - zpred)**2))\n",
    "print('RMS residual =', rms)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,12))\n",
    "scat = ax.scatter(x, y, c=zpred, vmin=0, vmax=zobs.max(), s=25, cmap='hot')\n",
    "fig.colorbar(scat)\n",
    "plt.savefig('Munich_fitted.jpg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#x, y = np.array(polyair.center_lon), np.array(polyair.center_lat)\n",
    "# Plot the 3D figure of the fitted function and the residuals.\n",
    "fig = plt.figure(figsize=(14,12))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(x, y, zpred, cmap='plasma', s=10, alpha=0.5)\n",
    "ax.scatter(x, y, zobs, color='green', s=5, alpha=0.4)\n",
    "ax.set_zlim(0,np.max(np.array(polyair.median_prices))+2)\n",
    "ax.view_init(35, 150)\n",
    "plt.show()\n",
    "\n",
    "alphas['Munich'] = np.mean(popt[6::7])\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_in = open(\"alphas.pickle\",\"rb\")\n",
    "alphas_dict = pickle.load(pickle_in)\n",
    "#prices_params_dict = {}\n",
    "alphas_dict['Munich'] = alphas['Munich']\n",
    "print(alphas_dict)\n",
    "pickle_out = open(\"alphas.pickle\",\"wb\")\n",
    "pickle.dump(alphas_dict, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.vstack([x, y])\n",
    "zobs = np.array(polyair.hmean_prices)\n",
    "#zobs = np.where(zobs>0, zobs, 0.1*zobs.max())\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _gaussian(M, *args):\n",
    "    xy = M\n",
    "    arr = np.zeros(len(zobs))\n",
    "    for i in range(len(args)//6):\n",
    "        arr += twoD_Gaussian(xy, *args[i*6:i*6+6])\n",
    "    return arr\n",
    "\n",
    "\n",
    "guess_prms = [(105.42, 19.375937,9.841730, 3, 2.5,0.1),\n",
    "              (105.15, 24.343842,11.321942,3, 3,1.3),\n",
    "              (165.49,22.356680, 8.361518, 2.5, 2,1.42),\n",
    "              (138.68,25.337423,7.868114, 3, 3,2.19),\n",
    "              (107.42,25.834213,4.907690, 1.48,0.72,7.48),\n",
    "                (119.15,19.375937,4.907690, 1.48,0.72,7.48)\n",
    "                #(170,24.49698,16.784035, 1.48,0.72,7.48, 1.3)\n",
    "             ]\n",
    "\n",
    "# Flatten the initial guess parameter list.\n",
    "p0 = [p for prms in guess_prms for p in prms]\n",
    "\n",
    "def twoD_Gaussian(xy, amplitude, xo, yo, sigma_x, sigma_y, theta):\n",
    "    x, y = xy\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)    \n",
    "    a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)\n",
    "    b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)\n",
    "    c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)\n",
    "    g = amplitude*np.exp( - (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo) \n",
    "                            + c*((y-yo)**2)))\n",
    "    return g.ravel()\n",
    "popt, pcov = opt.curve_fit(_gaussian, xy, zobs, p0)\n",
    "#pred_params, uncert_cov = opt.curve_fit(gauss2d, xy, zobs, p0=guess)\n",
    "\n",
    "zpred = np.zeros(len(zobs))\n",
    "for i in range(len(popt)//6):\n",
    "    zpred += twoD_Gaussian(xy, *popt[i*6:i*6+6])\n",
    "\n",
    "# for i in range(len(popt)//5):\n",
    "#     fit += gaussian(X, Y, *popt[i*5:i*5+5])\n",
    "print('Fitted parameters:')\n",
    "print(popt)\n",
    "\n",
    "rms = np.sqrt(np.mean((zobs - zpred)**2))\n",
    "print('RMS residual =', rms)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,12))\n",
    "scat = ax.scatter(x, y, c=zpred, vmin=0, vmax=zobs.max(), s=40, cmap='hot')\n",
    "fig.colorbar(scat)\n",
    "plt.savefig('Munich_fitted.jpg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#x, y = np.array(polyair.center_lon), np.array(polyair.center_lat)\n",
    "# Plot the 3D figure of the fitted function and the residuals.\n",
    "fig = plt.figure(figsize=(14,12))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(x, y, zpred, cmap='plasma', s=5, alpha=0.5)\n",
    "ax.scatter(x, y, zobs, color='green', s=2, alpha=0.2)\n",
    "ax.set_zlim(0,np.max(np.array(polyair.median_prices))+2)\n",
    "ax.view_init(25, 20)\n",
    "plt.show()\n",
    "print(popt[::6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "sqrt_eigs_long = np.array([])\n",
    "sqrt_eigs_short = np.array([])\n",
    "for i in range(0, len(popt), 6):\n",
    "    a = (np.cos(popt[i+5])**2)/(2*popt[i+3]**2) + (np.sin(popt[i+5])**2)/(2*popt[i+4]**2)\n",
    "    b = -(np.sin(2*popt[i+5]))/(4*popt[i+3]**2) + (np.sin(2*popt[i+5]))/(4*popt[i+4]**2)\n",
    "    c = (np.sin(popt[i+5])**2)/(2*popt[i+3]**2) + (np.cos(popt[i+5])**2)/(2*popt[i+4]**2)\n",
    "    cov = np.array([a, b, b, c]).reshape(-1, 2)\n",
    "    print(\"Is cov_{} positive definite?: \".format(i//6+1), is_pos_def(cov))\n",
    "    eigenvalues = np.linalg.eigvals(cov)\n",
    "    eigenvalues = eigenvalues[eigenvalues>0]\n",
    "    if eigenvalues.size!=0:\n",
    "        stds = np.sqrt(eigenvalues)/popt[i]\n",
    "        #stds = stds[stds>=0]\n",
    "        sqrt_eigs_long = np.append(sqrt_eigs_long,max(stds))\n",
    "        sqrt_eigs_short = np.append(sqrt_eigs_short,min(stds))\n",
    "\n",
    "print('long stds: ', sqrt_eigs_long, 'mean: ', np.mean(sqrt_eigs_long))\n",
    "print('short stds: ', sqrt_eigs_short, 'mean: ', np.mean(sqrt_eigs_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "polyframe_proj = polyframe.to_crs(city.crs)\n",
    "polyframe_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "polyair_proj = polyair.to_crs(city.crs)\n",
    "polyair_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyair_proj2 = polyair.to_crs(epsg=3857)\n",
    "polyframe_proj2 = polyframe.to_crs(epsg=3857)\n",
    "city2 = city.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyair_proj2[polyair_proj2.zpred>np.percentile(zpred,50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as ctx\n",
    "style = \"dark_all\"\n",
    "cartodb_url = 'https://a.basemaps.cartocdn.com/%s/tileZ/tileX/tileY.png' % style\n",
    "west, south, east, north = city2.unary_union.bounds\n",
    "fig, ax = plt.subplots(figsize=(30,25), dpi=90)\n",
    "#polyframe_proj2.plot(ax=ax, color='white', edgecolor='grey')\n",
    "polyair_proj2.plot(column='zpred', cmap='Blues', ax=ax, alpha=0.4) \n",
    "\n",
    "ctx.add_basemap(ax, url=ctx.tile_providers.ST_TONER_LITE, attribution=\"\",zoom='auto', alpha=0.5)\n",
    "ax.set_xlim(west, east)\n",
    "ax.set_ylim(south, north)\n",
    "ax.axis('off')\n",
    "#plt.title('Munich',fontsize= 30, color='grey')\n",
    "fig.savefig('Munich_airbnb.jpg', dpi=fig.dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "west, south, east, north = city2.unary_union.bounds\n",
    "cmap_reversed = plt.cm.get_cmap('cubehelix_r')\n",
    "#plt.rcParams.update({'font.size': 30}) \n",
    "fig, ax = plt.subplots(figsize=(30,25), dpi=90)\n",
    "# for polygon, n in zip(geometry_cut, np.arange(len(polylist))):\n",
    "#     p = polygon.representative_point().coords[:][0]\n",
    "#     patch = PolygonPatch(polygon, fc='#ffffff', ec='#000000', alpha=0.2)\n",
    "#     ax.add_patch(patch)\n",
    "#     #plt.annotate(s=n, xy=p,\n",
    "#                  #horizontalalignment='center', size=6)\n",
    "#polycounts.plot(ax = ax, column='counts', cmap=cmap_reversed, legend=True)  \n",
    "polyframe_proj2.plot(ax=ax, color='white', edgecolor='grey')\n",
    "polyair_proj2.plot(column='zpred', cmap='Blues', ax=ax) \n",
    "ax.set_xlim(west, east)\n",
    "ax.set_ylim(south, north)\n",
    "ax.axis('off')\n",
    "plt.title('Munich',fontsize= 30, color='grey')\n",
    "fig.savefig('Munich_airbnb2.jpg', dpi=fig.dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyair['zpred'] = zpred\n",
    "# plot the city\n",
    "west, south, east, north = city.unary_union.bounds\n",
    "fig, ax = plt.subplots(figsize=(40,20))\n",
    "polyframe_proj.plot(ax=ax, color='#000004')\n",
    "polyair_proj.plot(column='zpred',  legend=True, cmap='magma', ax=ax)    \n",
    "ax.set_xlim(west, east)\n",
    "ax.set_ylim(south, north)\n",
    "#ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_in = open(\"Prices_params.pickle\",\"rb\")\n",
    "prices_params_dict = pickle.load(pickle_in)\n",
    "#prices_params_dict = {}\n",
    "prices_params_dict['Munich'] = popt\n",
    "pickle_out = open(\"Prices_params.pickle\",\"wb\")\n",
    "pickle.dump(prices_params_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "#average_slopes = {}\n",
    "pickle_in_long = open(\"Average_price_slopes_long.pickle\",\"rb\")\n",
    "average_slopes_long = pickle.load(pickle_in_long)\n",
    "average_slopes_long['Munich'] = np.mean(sqrt_eigs_long)\n",
    "pickle_out_long = open(\"Average_price_slopes_long.pickle\",\"wb\")\n",
    "pickle.dump(average_slopes_long, pickle_out_long)\n",
    "pickle_out_long.close()\n",
    "\n",
    "pickle_in_short = open(\"Average_price_slopes_short.pickle\",\"rb\")\n",
    "average_slopes_short = pickle.load(pickle_in_short)\n",
    "average_slopes_short['Munich'] = np.mean(sqrt_eigs_short)\n",
    "pickle_out_short = open(\"Average_price_slopes_short.pickle\",\"wb\")\n",
    "pickle.dump(average_slopes_short, pickle_out_short)\n",
    "pickle_out_short.close()\n",
    "\n",
    "print('long: ', average_slopes_long, '\\n', ' short: ', average_slopes_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and project a street network\n",
    "G = ox.graph_from_place('Munich, Germany', network_type=\"walk\")\n",
    "#G = ox.project_graph(G)\n",
    "fig, ax = ox.plot_graph(G, fig_height=20, bgcolor='k', node_size=2, node_color='#999999', node_edgecolor='none', node_zorder=2,\n",
    "                        edge_color='#555555', edge_linewidth=0.5, edge_alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = {}\n",
    "for i in range(0, len(popt), 6):\n",
    "    lon = popt[i+1]*1000 + (np.array(polyair.center_lon)*500/meter_500).min()\n",
    "    lon = lon*meter_500/500\n",
    "    lat = popt[i+2]*1000 + (np.array(polyair.center_lat)*500/meter_500).min()\n",
    "    lat = lat*meter_500/500\n",
    "    centers['center_node_{}'.format(i//6+1)] = ox.get_nearest_node(G, (lat,lon))\n",
    "\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "#center_node = ox.get_nearest_node(G, (popt[8],popt[7]))\n",
    "center_node = centers['center_node_3']\n",
    "# list of distances from center\n",
    "dists = np.arange(500, 2000, 500)\n",
    "dists\n",
    "\n",
    "# get one color for each isochrone\n",
    "iso_colors = ox.get_colors(n=len(dists), cmap='Reds', start=0.3, return_hex=True)\n",
    "# color the nodes according to isochrone then plot the street network\n",
    "node_colors = {}\n",
    "for dist, color in zip(sorted(dists, reverse=True), iso_colors):\n",
    "    subgraph = nx.ego_graph(G, center_node, radius=dist, distance='length')\n",
    "    for node in subgraph.nodes():\n",
    "        node_colors[node] = color\n",
    "nc = [node_colors[node] if node in node_colors else 'none' for node in G.nodes()]\n",
    "ns = [20 if node in node_colors else 0 for node in G.nodes()]\n",
    "fig, ax = ox.plot_graph(G, fig_height=20,show=False, close=False, node_color=nc, node_size=ns, node_alpha=0.8, node_zorder=2)\n",
    "plt.close()\n",
    "# to this matplotlib axis, add the place shape as descartes polygon patches\n",
    "for geometry in city['geometry'].tolist():\n",
    "    if isinstance(geometry, (Polygon, MultiPolygon)):\n",
    "        if isinstance(geometry, Polygon):\n",
    "            geometry = MultiPolygon([geometry])\n",
    "        for polygon in geometry:\n",
    "            patch = PolygonPatch(polygon, fc='#cccccc', ec='k', linewidth=3, alpha=0.1, zorder=-1)\n",
    "            ax.add_patch(patch)\n",
    "            \n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "#poly_prices = polyair[polyair.median_prices>0]\n",
    "poly_prices = polyair.copy()\n",
    "print(len(poly_prices))\n",
    "poly_prices['zpred'] = zpred\n",
    "\n",
    "lorentz_vals = poly_prices.zpred.to_dict()\n",
    "s = [(k, lorentz_vals[k]) for k in sorted(lorentz_vals, key=lorentz_vals.get)]\n",
    "keys = []\n",
    "vals = []\n",
    "for k,v in s:\n",
    "    keys.append(k)\n",
    "    vals.append(v)\n",
    "    \n",
    "\n",
    "vals = np.array(vals)\n",
    "keys = np.array(keys)\n",
    "\n",
    "vals_cut = vals[vals>0.1*vals.max()]\n",
    "print(len(vals), len(vals_cut))\n",
    "\n",
    "L = np.cumsum(vals_cut)/np.sum(vals_cut)\n",
    "keys = keys[vals>0.1*vals.max()]\n",
    "print('Number of cells with price above 5th percentile: ', len(keys))\n",
    "\n",
    "\n",
    "\n",
    "mat = np.zeros(shape=(len(geometry_cut), len(geometry_cut)))\n",
    "for pair in tqdm_notebook(combinations(sorted(keys), 2)):\n",
    "    mat[pair[0], pair[1]] = geometry_cut[pair[0]].centroid.distance(geometry_cut[pair[1]].centroid)\n",
    "    \n",
    "print(mat)\n",
    "\n",
    "\n",
    "def isuppertriangular(M): \n",
    "    for i in range(1, len(M)): \n",
    "        for j in range(0, i): \n",
    "            if(M[i][j] != 0):  \n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "  \n",
    "if isuppertriangular(mat): \n",
    "    print (\"Yes\") \n",
    "else: \n",
    "    print (\"No\") \n",
    "    \n",
    "    \n",
    "L1= L\n",
    "F1 = np.arange(1, len(L1)+1)/len(L1)\n",
    "\n",
    "L1 = (L1 - L1.min())/(L1.max()-L1.min())\n",
    "\n",
    "from scipy import interpolate\n",
    "tck = interpolate.splrep(F1,L1)\n",
    "x0_1 =1\n",
    "y0_1 = interpolate.splev(x0_1,tck)\n",
    "dydx = interpolate.splev(x0_1,tck,der=1)\n",
    "tngnt1 = lambda x: dydx*x + (y0_1-dydx*x0_1)\n",
    "plt.plot(F1, L1)\n",
    "plt.plot(x0_1,y0_1, \"or\")\n",
    "plt.plot(F1[tngnt1(F1)>0],tngnt1(F1[tngnt1(F1)>0]), label=\"tangent\")\n",
    "plt.show()\n",
    "\n",
    "indlist = poly_prices.index.tolist()\n",
    "\n",
    "loubar_val = vals_cut[np.where(tngnt1(F1)>0)[0][0]]\n",
    "\n",
    "print('Loubar price: ', loubar_val)\n",
    "print('Average price: ', np.mean(vals_cut))\n",
    "\n",
    "if loubar_val > np.mean(vals_cut):\n",
    "    loubar_keys = keys[vals_cut>loubar_val]\n",
    "else:\n",
    "    loubar_keys = keys[vals_cut>np.mean(vals_cut)]\n",
    "#loubar_keys = keys[vals_cut>loubar_val]\n",
    "dist_mat = mat[keys.reshape(-1,1), keys]\n",
    "\n",
    "total_dist = dist_mat.sum()\n",
    "dist_corr = dist_mat[dist_mat>0]\n",
    "\n",
    "loubar_dist_mat = mat[loubar_keys.reshape(-1,1), loubar_keys]\n",
    "loubar_dist = loubar_dist_mat.sum()\n",
    "\n",
    "loubar_dist_corr = loubar_dist_mat[loubar_dist_mat>0]\n",
    "eta_loubar = loubar_dist_corr.mean()/dist_corr.mean()\n",
    "\n",
    "x = np.array(polyair.center_lon)*500/meter_500\n",
    "x = x-x.min()\n",
    "x = x/1000\n",
    "\n",
    "avg_dist_meters = (dist_corr.mean()/0.00899928)*1000\n",
    "print('average city distance: ', avg_dist_meters)\n",
    "print('eta = ', eta_loubar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle_in = open(\"City_spreading_index.pickle\",\"rb\")\n",
    "spreading_index_dict = pickle.load(pickle_in)\n",
    "#spreading_index_dict = {}\n",
    "spreading_index_dict['Munich'] = eta_loubar\n",
    "\n",
    "\n",
    "pickle_out = open(\"City_spreading_index.pickle\",\"wb\")\n",
    "pickle.dump(spreading_index_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "spreading_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"avg_distances_dict.pickle\",\"rb\")\n",
    "avg_distances_dict = pickle.load(pickle_in)\n",
    "#avg_distances_dict = {}\n",
    "avg_distances_dict['Munich'] = avg_dist_meters\n",
    "\n",
    "\n",
    "pickle_out = open(\"avg_distances_dict.pickle\",\"wb\")\n",
    "pickle.dump(avg_distances_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "avg_distances_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the city\n",
    "west, south, east, north = city.unary_union.bounds\n",
    "fig, ax = plt.subplots(figsize=(40,26))\n",
    "\n",
    "polyframe.plot(ax=ax, color='#000004')\n",
    "#polyair.plot(column='prices',  legend=True, cmap='magma', ax=ax)    \n",
    "poly_prices[poly_prices.index.isin(keys)].plot(ax=ax, column='zpred')\n",
    "ax.set_xlim(west, east)\n",
    "ax.set_ylim(south, north)\n",
    "#ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorentz_vals = poly_prices.zpred.to_dict()\n",
    "s = [(k, lorentz_vals[k]) for k in sorted(lorentz_vals, key=lorentz_vals.get)]\n",
    "keys = []\n",
    "vals = []\n",
    "for k,v in s:\n",
    "    keys.append(k)\n",
    "    vals.append(v)\n",
    "    \n",
    "print(len(keys))\n",
    "vals = np.array(vals)\n",
    "keys = np.array(keys)\n",
    "keys = keys[vals>=0.01*vals.max()]\n",
    "print(len(keys))\n",
    "\n",
    "indlist = poly_prices.index.tolist()\n",
    "mat = np.zeros(shape=(len(geometry_cut), len(geometry_cut)))\n",
    "for pair in tqdm_notebook(combinations(sorted(keys), 2)):\n",
    "    mat[pair[0], pair[1]] = geometry_cut[pair[0]].centroid.distance(geometry_cut[pair[1]].centroid)\n",
    "\n",
    "print(mat)\n",
    "\n",
    "\n",
    "def isuppertriangular(M): \n",
    "    for i in range(1, len(M)): \n",
    "        for j in range(0, i): \n",
    "            if(M[i][j] != 0):  \n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "\n",
    "if isuppertriangular(mat): \n",
    "    print (\"Yes\") \n",
    "else: \n",
    "    print (\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "#poly_prices = polyair[polyair.median_prices>0]\n",
    "poly_prices = polyair.copy()\n",
    "print(len(poly_prices))\n",
    "poly_prices['zpred'] = zpred\n",
    "\n",
    "lorentz_vals = poly_prices.zpred.to_dict()\n",
    "s = [(k, lorentz_vals[k]) for k in sorted(lorentz_vals, key=lorentz_vals.get)]\n",
    "keys = []\n",
    "vals = []\n",
    "for k,v in s:\n",
    "    keys.append(k)\n",
    "    vals.append(v)\n",
    "    \n",
    "\n",
    "vals = np.array(vals)\n",
    "keys_initial = np.array(keys)\n",
    "\n",
    "\n",
    "perc_vals = np.linspace(0.01, 1, 100)\n",
    "etas_cut = []\n",
    "for i in tqdm_notebook(perc_vals):\n",
    "    keys = keys_initial\n",
    "    vals_cut = vals[vals>=i*vals.max()]\n",
    "    #print(len(vals), len(vals_cut))\n",
    "\n",
    "    L = np.cumsum(vals_cut)/np.sum(vals_cut)\n",
    "    keys = keys[vals>=i*vals.max()]\n",
    "    #print('Number of cells with price above {}th percentile: '.format(i*100), len(keys))\n",
    "    \n",
    "    \n",
    "        \n",
    "    L1= L\n",
    "    F1 = np.arange(1, len(L1)+1)/len(L1)\n",
    "\n",
    "    L1 = (L1 - L1.min())/(L1.max()-L1.min())\n",
    "    \n",
    "\n",
    "    loubar_val = vals_cut[np.where(tngnt1(F1)>0)[0][0]]\n",
    "\n",
    "    #print('Loubar price: ', loubar_val)\n",
    "    #print('Average price: ', np.mean(vals_cut))\n",
    "    \n",
    "    \n",
    "    loubar_keys = keys[vals_cut>loubar_val]\n",
    "    dist_mat = mat[keys.reshape(-1,1), keys]\n",
    "\n",
    "    total_dist = dist_mat.sum()\n",
    "    dist_corr = dist_mat[dist_mat>0]\n",
    "\n",
    "    loubar_dist_mat = mat[loubar_keys.reshape(-1,1), loubar_keys]\n",
    "\n",
    "    loubar_dist = loubar_dist_mat.sum()\n",
    "\n",
    "    loubar_dist_corr = loubar_dist_mat[loubar_dist_mat>0]\n",
    "    eta = loubar_dist_corr.mean()/dist_corr.mean()\n",
    "    etas_cut.append(eta)\n",
    "    #print('eta = ', eta)\n",
    "etas_cut = np.array(etas_cut)\n",
    "etas_cut = np.where(np.isnan(etas_cut), 0, etas_cut)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(perc_vals, etas_cut, color='r', linestyle='--', marker='o', markersize=4, linewidth=1)\n",
    "plt.xlabel(\"Cutting threshold\")\n",
    "plt.ylabel(\"eta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"etas_cut_dict.pickle\",\"rb\")\n",
    "etas_cut_dict = pickle.load(pickle_in)\n",
    "#etas_cut_dict = {}\n",
    "etas_cut_dict['Munich'] = etas_cut\n",
    "\n",
    "\n",
    "pickle_out = open(\"etas_cut_dict.pickle\",\"wb\")\n",
    "pickle.dump(etas_cut_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "etas_cut_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for key in etas_cut_dict:\n",
    "    #print(len(etas_cut_dict[key]))\n",
    "    etas = etas_cut_dict[key]#[1:]\n",
    "    #vals = etas/etas[0]\n",
    "    plt.plot(np.linspace(0.01, 1, 100), etas, linestyle='-', linewidth=1)\n",
    "plt.xlabel(\"Price cutting threshold\")\n",
    "plt.ylabel(\"eta/eta_0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lval = np.linspace(0, max(vals_cut), 100)\n",
    "etas = []\n",
    "\n",
    "lorentz_vals = poly_prices.zpred.to_dict()\n",
    "s = [(k, lorentz_vals[k]) for k in sorted(lorentz_vals, key=lorentz_vals.get)]\n",
    "keys = []\n",
    "vals = []\n",
    "for k,v in s:\n",
    "    keys.append(k)\n",
    "    vals.append(v)\n",
    "    \n",
    "\n",
    "vals = np.array(vals)\n",
    "keys = np.array(keys)\n",
    "vals_cut = vals[vals>0.03*vals.max()]\n",
    "print(len(vals), len(vals_cut))\n",
    "\n",
    "L = np.cumsum(vals_cut)/np.sum(vals_cut)\n",
    "keys = keys[vals>0.03*vals.max()]\n",
    "print('Number of cells with price above 5th percentile: ', len(keys))\n",
    "\n",
    "for i in tqdm_notebook(lval):\n",
    "    loubar_keys = keys[vals_cut>=i]\n",
    "    dist_mat = mat[keys.reshape(-1,1), keys]\n",
    "\n",
    "    total_dist = dist_mat.sum()\n",
    "    dist_corr = dist_mat[dist_mat>0]\n",
    "\n",
    "    loubar_dist_mat = mat[loubar_keys.reshape(-1,1), loubar_keys]\n",
    "\n",
    "    loubar_dist = loubar_dist_mat.sum()\n",
    "\n",
    "    loubar_dist_corr = loubar_dist_mat[loubar_dist_mat>0]\n",
    "    eta = loubar_dist_corr.mean()/dist_corr.mean()\n",
    "    etas.append(eta)\n",
    "    #print('eta = ', eta)\n",
    "etas = np.array(etas)\n",
    "etas = np.where(np.isnan(etas), 0, etas)\n",
    "lval = lval/lval.max()\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(lval, etas, color='r', linestyle='--', marker='o', markersize=4, linewidth=1)\n",
    "plt.xlabel(\"Price threshold\")\n",
    "plt.ylabel(\"eta\")\n",
    "plt.show()\n",
    "\n",
    "etas_cut_threshold = etas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"etas_cut_threshold_dict.pickle\",\"rb\")\n",
    "etas_cut_threshold_dict = pickle.load(pickle_in)\n",
    "#etas_cut_threshold_dict = {}\n",
    "etas_cut_threshold_dict['Munich'] = etas_cut_threshold\n",
    "\n",
    "\n",
    "pickle_out = open(\"etas_cut_threshold_dict.pickle\",\"wb\")\n",
    "pickle.dump(etas_cut_threshold_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "etas_cut_threshold_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for key in etas_cut_threshold_dict:\n",
    "    plt.plot(np.arange(100), etas_cut_threshold_dict[key], linestyle='-', linewidth=1)\n",
    "plt.xlabel(\"Price threshold\")\n",
    "plt.ylabel(\"eta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorentz_vals = poly_prices.zpred.to_dict()\n",
    "s = [(k, lorentz_vals[k]) for k in sorted(lorentz_vals, key=lorentz_vals.get)]\n",
    "keys = []\n",
    "vals = []\n",
    "for k,v in s:\n",
    "    keys.append(k)\n",
    "    vals.append(v)\n",
    "    \n",
    "\n",
    "vals = np.array(vals)\n",
    "#keys = np.array(keys)\n",
    "keys_initial = np.array(keys)\n",
    "\n",
    "\n",
    "perc_vals = np.linspace(0.01, 1, 100)\n",
    "etas_2d = []\n",
    "for i in tqdm_notebook(perc_vals):\n",
    "    keys = keys_initial\n",
    "    vals_cut = vals[vals>=i*vals.max()]\n",
    "    #print(len(vals), len(vals_cut))\n",
    "\n",
    "    L = np.cumsum(vals_cut)/np.sum(vals_cut)\n",
    "    keys = keys[vals>=i*vals.max()]\n",
    "    #print('Number of cells with price above {}th percentile: '.format(i*100), len(keys))\n",
    "    etas = []\n",
    "    lval = np.linspace(min(vals_cut), max(vals_cut), 100)\n",
    "    for k in tqdm_notebook(lval):\n",
    "        loubar_keys = keys[vals_cut>=k]\n",
    "        dist_mat = mat[keys.reshape(-1,1), keys]\n",
    "\n",
    "        total_dist = dist_mat.sum()\n",
    "        dist_corr = dist_mat[dist_mat>0]\n",
    "\n",
    "        loubar_dist_mat = mat[loubar_keys.reshape(-1,1), loubar_keys]\n",
    "\n",
    "        loubar_dist = loubar_dist_mat.sum()\n",
    "\n",
    "        loubar_dist_corr = loubar_dist_mat[loubar_dist_mat>0]\n",
    "        eta = loubar_dist_corr.mean()/dist_corr.mean()\n",
    "        etas.append(eta)\n",
    "        #print('eta = ', eta)\n",
    "    etas_array = np.array(etas)\n",
    "    etas_array = np.where(np.isnan(etas_array), 0, etas_array)\n",
    "    lval = (lval - lval.min())/(lval - lval.min()).max()\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(lval, etas_array, color='r', linestyle='--', marker='o', markersize=4, linewidth=1)\n",
    "    plt.xlabel(\"Price threshold\")\n",
    "    plt.ylabel(\"eta\")\n",
    "    plt.show()\n",
    "    etas_2d.append(etas)\n",
    "etas_2d = np.array(etas_2d)\n",
    "etas_2d = np.where(np.isnan(etas_2d), 0, etas_2d)\n",
    "lval = lval/lval.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = np.meshgrid(np.linspace(0,1, 100),np.linspace(0,1, 100))\n",
    "etas_surface = etas_2d\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# Plot a 3D surface\n",
    "ax.plot_surface(X, Y, etas_surface)\n",
    "ax.set_xlabel('Price treshold')\n",
    "ax.set_ylabel('Boundary cutoff threshold')\n",
    "ax.set_zlabel('Spreading index')\n",
    "ax.view_init(45, 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"etas_surface_dict.pickle\",\"rb\")\n",
    "etas_surface_dict = pickle.load(pickle_in)\n",
    "#etas_surface_dict = {}\n",
    "etas_surface_dict['Munich'] = etas_surface\n",
    "\n",
    "\n",
    "pickle_out = open(\"etas_surface_dict.pickle\",\"wb\")\n",
    "pickle.dump(etas_surface_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "etas_surface_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "lorentz_vals = poly_prices.zpred.to_dict()\n",
    "s = [(k, lorentz_vals[k]) for k in sorted(lorentz_vals, key=lorentz_vals.get)]\n",
    "keys = []\n",
    "vals = []\n",
    "for k,v in s:\n",
    "    keys.append(k)\n",
    "    vals.append(v)\n",
    "    \n",
    "\n",
    "vals = np.array(vals)\n",
    "keys = np.array(keys)\n",
    "\n",
    "vals_cut = vals[vals>0.03*vals.max()]\n",
    "print(len(vals), len(vals_cut))\n",
    "\n",
    "L = np.cumsum(vals_cut)/np.sum(vals_cut)\n",
    "keys = keys[vals>0.03*vals.max()]\n",
    "print('Number of cells with price above 5th percentile: ', len(keys))\n",
    "\n",
    "print('total number of cells: ', len(polylist))\n",
    "\n",
    "times_names = ['times_{}'.format(c) for c in range(1, len(centers)+1)]\n",
    "times_dict = {name:[] for name in times_names}\n",
    "\n",
    "plist = poly_prices[poly_prices.index.isin(keys)].geometry.tolist()\n",
    "print('number of cells within urban airbnb territory: ', len(plist))\n",
    "for poly in tqdm_notebook(plist):\n",
    "    origin = ox.get_nearest_node(G, poly.centroid.coords[0][::-1])\n",
    "    dists_to_centers = []\n",
    "    for node, target in centers.items():\n",
    "        try:\n",
    "            if origin!=target:\n",
    "                path = nx.shortest_path(G, origin, target, weight='length')\n",
    "                edges = [ tuple( path[i:i+2] ) for i in range( len(path) - 1 ) ]\n",
    "                dist = np.sum([G.get_edge_data(*edge)[0]['length'] for edge in edges])\n",
    "                dists_to_centers.append(dist)\n",
    "        except:\n",
    "            pass\n",
    "    if len(dists_to_centers) != 0:\n",
    "        dists_to_centers = sorted(dists_to_centers)\n",
    "        #print('distance list length equal to # of centers: ', len(dists_to_centers)==len(centers))\n",
    "        if len(dists_to_centers)==len(centers):\n",
    "            for n, dist in enumerate(dists_to_centers):\n",
    "                time = (dist/3.6)/60\n",
    "                times_dict['times_{}'.format(n+1)].append(time)\n",
    "                #print(times_dict['times_{}'.format(n+1)][-1])\n",
    "        else:\n",
    "            print('Distance list length NOT equal to # of centers')\n",
    "\n",
    "#print(distances_dict)\n",
    "for center in range(1, len(centers)+1):\n",
    "    print(\"Mean travel times in minutes: \", np.mean(times_dict['times_{}'.format(center)]))\n",
    "for center in range(1, len(centers)+1):\n",
    "    plt.hist(times_dict['times_{}'.format(center)], bins=20, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for center in range(1, len(centers)+1):\n",
    "    times_dict['times_{}'.format(center)] = np.array(times_dict['times_{}'.format(center)])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "for center in range(1, len(centers)+1)[::-1]:\n",
    "    plt.hist(times_dict['times_{}'.format(center)], bins=20, alpha=0.8, density=True, label='to {} closest center'.format(center))\n",
    "plt.title('Travel time distributions to closest centers in Munich')\n",
    "plt.xlabel('Minutes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "for center in range(1, len(centers)+1):\n",
    "    data = times_dict['times_{}'.format(center)]\n",
    "    ecdf = ECDF(data)\n",
    "    plt.scatter(ecdf.x, ecdf.y, alpha=0.8, marker='x', s=8, label='to {} closest center'.format(center))\n",
    "plt.title('Travel time distributions to closest centers in Munich')\n",
    "plt.xlabel('Minutes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"City_Accessibility.pickle\",\"rb\")\n",
    "access_dict = pickle.load(pickle_in)\n",
    "#access_dict = {}\n",
    "access_dict['Munich'] = times_dict\n",
    "\n",
    "\n",
    "pickle_out = open(\"City_Accessibility.pickle\",\"wb\")\n",
    "pickle.dump(access_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "access_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = times_dict['times_{}'.format(1)]\n",
    "y = times_dict['times_{}'.format(2)]\n",
    "c_xy = np.histogram2d(x, y, 20)[0]\n",
    "c_xx = np.histogram2d(x, x, 20)[0]\n",
    "plt.imshow(c_xy, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def calc_MI(x, y, bins):\n",
    "    c_xy = np.histogram2d(x, y, bins)[0]\n",
    "    mi = mutual_info_score(None, None, contingency=c_xy)\n",
    "    return mi\n",
    "\n",
    "mi_list = []\n",
    "for center in range(1, len(centers)+1):\n",
    "    mi_list.append(times_dict['times_{}'.format(center)])\n",
    "\n",
    "mis = []\n",
    "for pair in combinations_with_replacement(mi_list, 2):\n",
    "    MI = calc_MI(pair[0], pair[1], 20)\n",
    "    print(MI)\n",
    "    mis.append(MI)\n",
    "mis = np.array(mis)\n",
    "avg_mi = np.mean(mis)\n",
    "print('average mutual information = ', avg_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"Time_distribution_mutual_information.pickle\",\"rb\")\n",
    "MI_dict = pickle.load(pickle_in)\n",
    "#MI_dict = {}\n",
    "MI_dict['Munich'] = avg_mi\n",
    "\n",
    "\n",
    "pickle_out = open(\"Time_distribution_mutual_information.pickle\",\"wb\")\n",
    "pickle.dump(MI_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "MI_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
